[2024-03-26 06:14:01 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-26 06:14:01 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-26 06:14:01 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-26 06:14:01 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-26 06:14:01 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-26 06:14:01 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-26 06:14:01 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-26 06:14:01 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-26 06:14:01 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-26 06:14:01 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-26 06:14:01 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-26 06:14:01 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-26 06:14:05 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-26 06:14:14 seg] (vit_seg.py 139): INFO Building ViTSegInference with 60 classes, test_cfg=Config (path: None): {'bg_thresh': 0.25, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-26 06:30:39 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-26 06:30:39 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-26 06:30:39 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-26 06:30:39 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-26 06:30:39 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-26 06:30:39 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-26 06:30:39 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-26 06:30:39 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-26 06:30:39 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-26 06:30:39 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-26 06:30:39 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-26 06:30:39 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-26 06:30:43 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-26 06:30:49 seg] (vit_seg.py 139): INFO Building ViTSegInference with 60 classes, test_cfg=Config (path: None): {'bg_thresh': 0.25, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-26 07:14:06 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-26 07:14:06 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-26 07:14:06 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-26 07:14:06 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-26 07:14:06 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-26 07:14:06 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-26 07:14:06 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-26 07:14:06 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-26 07:14:06 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-26 07:14:06 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-26 07:14:06 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-26 07:14:06 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-26 07:14:11 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-26 07:14:22 seg] (vit_seg.py 139): INFO Building ViTSegInference with 60 classes, test_cfg=Config (path: None): {'bg_thresh': 0.25, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-26 07:27:50 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-26 07:27:50 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-26 07:27:50 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-26 07:27:50 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-26 07:27:50 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-26 07:27:50 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-26 07:27:50 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-26 07:27:50 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-26 07:27:50 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-26 07:27:50 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-26 07:27:50 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-26 07:27:50 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-26 07:28:17 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-26 07:28:24 seg] (vit_seg.py 139): INFO Building ViTSegInference with 60 classes, test_cfg=Config (path: None): {'bg_thresh': 0.25, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-26 09:08:09 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-26 09:08:09 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-26 09:08:09 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-26 09:08:09 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-26 09:08:09 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-26 09:08:09 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-26 09:08:09 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-26 09:08:09 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-26 09:08:09 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-26 09:08:09 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-26 09:08:09 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-26 09:08:09 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-26 09:08:22 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-26 09:08:26 seg] (vit_seg.py 139): INFO Building ViTSegInference with 60 classes, test_cfg=Config (path: None): {'bg_thresh': 0.25, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-26 10:41:31 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-26 10:41:31 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-26 10:41:31 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-26 10:41:31 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-26 10:41:31 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-26 10:41:31 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-26 10:41:31 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-26 10:41:31 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-26 10:41:31 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-26 10:41:31 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-26 10:41:31 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-26 10:41:31 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-26 10:41:34 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-26 10:41:39 seg] (vit_seg.py 139): INFO Building ViTSegInference with 60 classes, test_cfg=Config (path: None): {'bg_thresh': 0.25, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-26 11:19:29 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-26 11:19:29 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-26 11:19:29 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-26 11:19:29 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-26 11:19:29 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-26 11:19:29 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-26 11:19:29 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-26 11:19:29 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-26 11:19:29 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-26 11:19:29 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-26 11:19:29 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-26 11:19:29 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-26 11:19:33 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-26 11:19:37 seg] (vit_seg.py 139): INFO Building ViTSegInference with 60 classes, test_cfg=Config (path: None): {'bg_thresh': 0.25, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-26 13:27:34 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-26 13:27:34 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-26 13:27:34 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-26 13:27:34 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-26 13:27:34 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-26 13:27:34 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-26 13:27:34 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-26 13:27:34 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-26 13:27:34 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-26 13:27:34 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-26 13:27:34 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-26 13:27:34 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-26 13:27:37 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-26 13:27:42 seg] (vit_seg.py 139): INFO Building ViTSegInference with 81 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-26 13:29:31 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-26 13:29:31 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-26 13:29:31 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-26 13:29:31 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-26 13:29:31 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-26 13:29:31 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-26 13:29:31 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-26 13:29:31 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-26 13:29:31 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-26 13:29:31 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-26 13:29:31 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-26 13:29:31 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-26 13:29:35 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-26 13:29:39 seg] (vit_seg.py 139): INFO Building ViTSegInference with 82 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-27 07:38:12 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 07:38:12 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 07:38:12 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 07:38:12 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 07:38:12 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 07:38:12 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 07:38:12 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 07:38:12 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 07:38:12 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 07:38:12 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 07:38:12 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 07:38:12 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 07:38:17 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 07:38:22 seg] (vit_seg.py 139): INFO Building ViTSegInference with 82 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-27 07:41:15 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 07:41:15 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 07:41:15 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 07:41:15 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 07:41:15 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 07:41:15 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 07:41:15 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 07:41:15 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 07:41:15 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 07:41:15 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 07:41:15 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 07:41:15 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 07:41:19 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 07:41:24 seg] (vit_seg.py 139): INFO Building ViTSegInference with 82 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-27 08:25:42 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 08:25:42 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 08:25:42 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 08:25:42 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 08:25:42 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 08:25:42 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 08:25:42 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 08:25:42 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 08:25:42 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 08:25:42 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 08:25:42 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 08:25:42 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 08:25:47 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 08:25:52 seg] (vit_seg.py 139): INFO Building ViTSegInference with 82 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-27 08:28:53 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 08:28:53 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 08:28:53 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 08:28:53 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 08:28:53 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 08:28:53 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 08:28:53 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 08:28:53 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 08:28:53 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 08:28:53 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 08:28:53 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 08:28:53 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 08:28:58 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 08:29:03 seg] (vit_seg.py 139): INFO Building ViTSegInference with 82 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-27 08:37:13 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 08:37:13 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 08:37:13 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 08:37:13 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 08:37:13 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 08:37:13 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 08:37:13 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 08:37:13 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 08:37:13 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 08:37:13 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 08:37:13 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 08:37:13 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 08:37:17 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 08:37:22 seg] (vit_seg.py 139): INFO Building ViTSegInference with 82 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-27 08:39:48 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 08:39:48 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 08:39:48 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 08:39:48 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 08:39:48 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 08:39:48 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 08:39:48 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 08:39:48 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 08:39:48 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 08:39:48 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 08:39:48 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 08:39:48 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 08:39:52 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 08:39:58 seg] (vit_seg.py 139): INFO Building ViTSegInference with 82 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-27 08:46:52 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 08:46:52 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 08:46:52 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 08:46:52 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 08:46:52 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 08:46:52 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 08:46:52 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 08:46:52 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 08:46:52 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 08:46:52 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 08:46:52 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 08:46:52 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 08:46:57 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 08:47:02 seg] (vit_seg.py 139): INFO Building ViTSegInference with 82 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-27 08:50:15 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 08:50:15 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 08:50:15 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 08:50:15 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 08:50:15 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 08:50:15 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 08:50:15 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 08:50:15 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 08:50:15 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 08:50:15 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 08:50:15 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 08:50:15 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 08:50:19 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 08:50:25 seg] (vit_seg.py 139): INFO Building ViTSegInference with 82 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-27 08:54:45 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 08:54:45 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 08:54:45 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 08:54:45 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 08:54:45 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 08:54:45 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 08:54:45 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 08:54:45 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 08:54:45 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 08:54:45 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 08:54:45 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 08:54:45 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 08:54:49 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 08:54:54 seg] (vit_seg.py 139): INFO Building ViTSegInference with 82 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-27 09:09:04 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 09:09:04 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 09:09:04 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 09:09:04 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 09:09:04 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 09:09:04 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 09:09:04 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 09:09:04 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 09:09:04 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 09:09:04 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 09:09:04 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 09:09:04 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 09:09:09 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 09:09:14 seg] (vit_seg.py 139): INFO Building ViTSegInference with 82 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-27 09:09:53 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 09:09:53 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 09:09:53 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 09:09:53 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 09:09:53 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 09:09:53 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 09:09:53 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 09:09:53 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 09:09:53 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 09:09:53 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 09:09:53 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 09:09:53 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 09:09:57 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 09:10:03 seg] (vit_seg.py 139): INFO Building ViTSegInference with 82 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=True
[2024-03-27 09:59:09 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 09:59:09 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 09:59:09 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 09:59:09 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 09:59:09 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 09:59:09 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 09:59:09 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 09:59:09 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 09:59:09 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 09:59:09 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 09:59:09 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 09:59:09 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 09:59:14 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 09:59:20 seg] (vit_seg.py 139): INFO Building ViTSegInference with 3 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=False
[2024-03-27 10:00:27 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 10:00:27 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 10:00:27 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 10:00:27 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 10:00:27 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 10:00:27 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 10:00:27 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 10:00:27 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 10:00:27 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 10:00:27 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 10:00:27 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 10:00:27 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 10:00:31 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 10:00:37 seg] (vit_seg.py 139): INFO Building ViTSegInference with 2 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=False
[2024-03-27 10:03:28 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 10:03:28 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 10:03:28 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 10:03:28 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 10:03:28 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 10:03:28 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 10:03:28 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 10:03:28 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 10:03:28 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 10:03:28 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 10:03:28 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 10:03:28 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 10:03:32 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 10:03:38 seg] (vit_seg.py 139): INFO Building ViTSegInference with 2 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=False
[2024-03-27 10:06:12 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 10:06:12 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 10:06:12 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 10:06:12 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 10:06:12 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 10:06:12 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 10:06:12 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 10:06:12 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 10:06:12 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 10:06:12 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 10:06:12 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 10:06:12 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 10:06:16 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 10:06:21 seg] (vit_seg.py 139): INFO Building ViTSegInference with 2 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=False
[2024-03-27 10:13:26 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 10:13:26 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 10:13:26 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 10:13:26 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 10:13:26 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 10:13:26 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 10:13:26 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 10:13:26 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 10:13:26 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 10:13:26 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 10:13:26 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 10:13:26 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 10:13:29 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 10:13:35 seg] (vit_seg.py 139): INFO Building ViTSegInference with 3 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=False
[2024-03-27 11:50:25 seg] (util_module.py 194): INFO 	 embed_dim: 512
[2024-03-27 11:50:25 seg] (util_module.py 194): INFO 	 image_resolution: 224
[2024-03-27 11:50:25 seg] (util_module.py 194): INFO 	 vision_layers: 12
[2024-03-27 11:50:25 seg] (util_module.py 194): INFO 	 vision_width: 768
[2024-03-27 11:50:25 seg] (util_module.py 194): INFO 	 vision_patch_size: 16
[2024-03-27 11:50:25 seg] (util_module.py 194): INFO 	 context_length: 77
[2024-03-27 11:50:25 seg] (util_module.py 194): INFO 	 vocab_size: 49408
[2024-03-27 11:50:25 seg] (util_module.py 194): INFO 	 transformer_width: 512
[2024-03-27 11:50:25 seg] (util_module.py 194): INFO 	 transformer_heads: 8
[2024-03-27 11:50:25 seg] (util_module.py 194): INFO 	 transformer_layers: 12
[2024-03-27 11:50:25 seg] (util_module.py 194): INFO 		 first_stage_layer: 10
[2024-03-27 11:50:25 seg] (util_module.py 194): INFO 	 cut_top_layer: 0
[2024-03-27 11:50:29 seg] (util_module.py 141): WARNING Weights from pretrained model not used in SegCLIP: 
   vis_mae_decoder.mask_token
   vis_mae_decoder.decoder_pos_embed
   vis_mae_decoder.decoder_embed.weight
   vis_mae_decoder.decoder_embed.bias
   vis_mae_decoder.decoder_blocks.0.norm1.weight
   vis_mae_decoder.decoder_blocks.0.norm1.bias
   vis_mae_decoder.decoder_blocks.0.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.0.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.0.attn.proj.weight
   vis_mae_decoder.decoder_blocks.0.attn.proj.bias
   vis_mae_decoder.decoder_blocks.0.norm2.weight
   vis_mae_decoder.decoder_blocks.0.norm2.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.0.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.1.norm1.weight
   vis_mae_decoder.decoder_blocks.1.norm1.bias
   vis_mae_decoder.decoder_blocks.1.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.1.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.1.attn.proj.weight
   vis_mae_decoder.decoder_blocks.1.attn.proj.bias
   vis_mae_decoder.decoder_blocks.1.norm2.weight
   vis_mae_decoder.decoder_blocks.1.norm2.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.1.mlp.fc2.bias
   vis_mae_decoder.decoder_blocks.2.norm1.weight
   vis_mae_decoder.decoder_blocks.2.norm1.bias
   vis_mae_decoder.decoder_blocks.2.attn.qkv.weight
   vis_mae_decoder.decoder_blocks.2.attn.qkv.bias
   vis_mae_decoder.decoder_blocks.2.attn.proj.weight
   vis_mae_decoder.decoder_blocks.2.attn.proj.bias
   vis_mae_decoder.decoder_blocks.2.norm2.weight
   vis_mae_decoder.decoder_blocks.2.norm2.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc1.bias
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.weight
   vis_mae_decoder.decoder_blocks.2.mlp.fc2.bias
   vis_mae_decoder.decoder_norm.weight
   vis_mae_decoder.decoder_norm.bias
   vis_mae_decoder.decoder_pred.weight
   vis_mae_decoder.decoder_pred.bias
[2024-03-27 11:50:34 seg] (vit_seg.py 139): INFO Building ViTSegInference with 2 classes, test_cfg=Config (path: None): {'bg_thresh': 0.65, 'mode': 'slide', 'stride': (224, 224), 'crop_size': (224, 224)}, with_bg=False
